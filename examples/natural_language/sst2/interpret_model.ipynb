{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from path_explain import utils\n",
    "utils.set_up_environment(visible_devices='2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import scipy\n",
    "from bert_explainer import BertExplainerTF\n",
    "from transformers import *\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'sst-2'\n",
    "num_labels = len(glue_processors[task]().get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('.', num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('.', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/homes/gws/psturm/tensorflow_datasets/glue/sst2/0.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /homes/gws/psturm/tensorflow_datasets/glue/sst2/0.0.2\n"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load('glue/sst2', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task=task)\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task=task)\n",
    "valid_dataset = valid_dataset.batch(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = model.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = []\n",
    "for batch in valid_dataset:\n",
    "    valid_labels.append(batch[1].numpy())\n",
    "valid_labels = np.concatenate(valid_labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.8956\n",
      "Positive Sentiment Accuracy: 0.9527\n",
      "Negative Sentiment Accuracy: 0.8364\n"
     ]
    }
   ],
   "source": [
    "valid_pred_max = np.argmax(valid_pred, axis=-1)\n",
    "accuracy = np.sum(valid_pred_max == valid_labels) / len(valid_labels)\n",
    "\n",
    "positive_mask = valid_labels == 1\n",
    "positive_accuracy = np.sum(valid_pred_max[positive_mask] == valid_labels[positive_mask]) / np.sum(positive_mask)\n",
    "\n",
    "negative_mask = valid_labels == 0\n",
    "negative_accuracy = np.sum(valid_pred_max[negative_mask] == valid_labels[negative_mask]) / np.sum(negative_mask)\n",
    "\n",
    "print('Validation Accuracy: {:.4f}'.format(accuracy))\n",
    "print('Positive Sentiment Accuracy: {:.4f}'.format(positive_accuracy))\n",
    "print('Negative Sentiment Accuracy: {:.4f}'.format(negative_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] pumpkin takes an admirable look at the hypocrisy of political correctness, but it does so with such an uneven tone that you never know when humor ends and tragedy begins. [SEP]\n",
      "This sentence is negative (predicted confidence: 0.5426)\n",
      "-------------------------\n",
      "[CLS] is the time really ripe for a warmed - over james bond adventure, with a village idiot as the 007 clone? [SEP]\n",
      "This sentence is negative (predicted confidence: 0.8609)\n",
      "-------------------------\n",
      "[CLS] exquisitely nuanced in mood tics and dialogue, this chamber drama is superbly acted by the deeply appealing veteran bouquet and the chilling but quite human berling. [SEP]\n",
      "This sentence is positive (predicted confidence: 0.9992)\n",
      "-------------------------\n",
      "[CLS] the movie's relatively simple plot and uncomplicated morality play well with the affable cast. [SEP]\n",
      "This sentence is positive (predicted confidence: 0.9983)\n",
      "-------------------------\n",
      "[CLS] dense with characters and contains some thrilling moments. [SEP]\n",
      "This sentence is positive (predicted confidence: 0.9985)\n",
      "-------------------------\n",
      "[CLS] stephen rea, aidan quinn, and alan bates play desmond's legal eagles, and when joined by brosnan, the sight of this grandiloquent quartet lolling in pretty irish settings is a pleasant enough thing, ` tis. [SEP]\n",
      "This sentence is positive (predicted confidence: 0.9985)\n",
      "-------------------------\n",
      "[CLS] a marvel like none you've seen. [SEP]\n",
      "This sentence is positive (predicted confidence: 0.9960)\n",
      "-------------------------\n",
      "[CLS] does paint some memorable images..., but makhmalbaf keeps her distance from the characters [SEP]\n",
      "This sentence is positive (predicted confidence: 0.8664)\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_conf = valid_pred[-8:]\n",
    "batch_conf = scipy.special.softmax(batch_conf, axis=-1)\n",
    "for i in range(batch[0]['input_ids'].shape[0]):\n",
    "    encoded_sentence = batch[0]['input_ids'].numpy()[i]\n",
    "    encoded_sentence = encoded_sentence[encoded_sentence != 0]\n",
    "    label = batch[1][i].numpy()\n",
    "    print(tokenizer.decode(encoded_sentence))\n",
    "    print('This sentence is {} (predicted confidence: {:.4f})'.format('positive' if label == 1 else 'negative', batch_conf[i, label]))\n",
    "    print('-------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = BertExplainerTF(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_ids = tf.cast(batch[0]['input_ids'], tf.float64)\n",
    "batch_labels   = batch[1]\n",
    "batch_baseline = np.zeros((1, 128)).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 4\n",
    "mask = batch_input_ids[index] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-140.07 -288.91 72.79 -62.19 76.10 -60.76 87.47 -163.04 50.96 -27.59 -0.30 7.17 "
     ]
    }
   ],
   "source": [
    "for a in attributions[index][mask]:\n",
    "    print('{:.2f}'.format(a), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-448.3723403776483"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(attributions[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [00:04<00:28,  4.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [00:07<00:23,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [00:10<00:18,  3.71s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [00:14<00:14,  3.60s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [00:17<00:10,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [00:20<00:06,  3.47s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [00:24<00:03,  3.43s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [00:27<00:00,  3.44s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "attributions = explainer.attributions(inputs=batch_input_ids,\n",
    "                                      baseline=batch_baseline,\n",
    "                                      batch_size=10,\n",
    "                                      num_samples=100,\n",
    "                                      use_expectation=False,\n",
    "                                      output_indices=batch_labels,\n",
    "                                      verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Custom Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was too long, but overall a good movie nonetheless\n",
      "The above sentence is positive with confidence: 0.9545\n",
      "-------------------\n",
      "This movie was not bad\n",
      "The above sentence is positive with confidence: 0.9485\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "sentences = [(0, 'This movie was too long, but overall a good movie nonetheless'),\n",
    "             (1, 'This movie was not bad')]\n",
    "input_sen = [transformers.data.InputExample(guid=x[0],\n",
    "                                            text_a=x[1],\n",
    "                                            label='0') for x in sentences]\n",
    "examples = glue_convert_examples_to_features(input_sen,\n",
    "                                             tokenizer,\n",
    "                                             max_length=128,\n",
    "                                             task=task)\n",
    "input_ids = np.array([example.input_ids for example in examples])\n",
    "predicted_logits = model(input_ids)[0]\n",
    "predicted_confidence = scipy.special.softmax(predicted_logits, axis=-1)\n",
    "predicted_labels = np.argmax(predicted_confidence, axis=1)\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i][1])\n",
    "    print('The above sentence is {} with confidence: {:.4f}'.format(\n",
    "        'positive' if predicted_logits[i, 1] > predicted_logits[i, 0] else 'negative',\n",
    "        predicted_confidence[i, predicted_labels[i]]\n",
    "    ))\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting model output\n",
    "Note that there is a difference between specifying and not specifying the attention mask.\n",
    "The explanation code does specify the attention mask as the nonzero entries, but\n",
    "I need to confirm that doing so makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=241221, shape=(8, 2), dtype=float32, numpy=\n",
       " array([[-0.08151681, -0.25236744],\n",
       "        [ 0.7073199 , -1.1151597 ],\n",
       "        [-3.7333772 ,  3.39218   ],\n",
       "        [-3.397447  ,  2.9782803 ],\n",
       "        [-3.4761655 ,  3.0213773 ],\n",
       "        [-3.454523  ,  3.0430386 ],\n",
       "        [-3.043965  ,  2.4792337 ],\n",
       "        [-1.3244063 ,  0.5451266 ]], dtype=float32)>,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=239074, shape=(8, 2), dtype=float32, numpy=\n",
       "array([[ 0.24352185, -0.59544015],\n",
       "       [ 0.70060503, -1.0981779 ],\n",
       "       [-3.30811   ,  2.8154087 ],\n",
       "       [-2.380083  ,  1.7778169 ],\n",
       "       [-1.4645485 ,  0.79111874],\n",
       "       [-2.7737343 ,  2.2790208 ],\n",
       "       [-1.4757023 ,  0.7624265 ],\n",
       "       [-0.8581508 ,  0.23136781]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(batch_input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ids = tf.cast(batch_input_ids, tf.int32)\n",
    "batch_masks = tf.cast(tf.cast(input_ids, tf.bool), tf.int32)\n",
    "batch_token_types = tf.zeros(batch_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=245520, shape=(8, 2), dtype=float32, numpy=\n",
       "array([[-0.08151681, -0.25236744],\n",
       "       [ 0.7073199 , -1.1151597 ],\n",
       "       [-3.7333772 ,  3.39218   ],\n",
       "       [-3.397447  ,  2.9782803 ],\n",
       "       [-3.4761655 ,  3.0213773 ],\n",
       "       [-3.454523  ,  3.0430386 ],\n",
       "       [-3.043965  ,  2.4792337 ],\n",
       "       [-1.3244063 ,  0.5451266 ]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model([batch_ids, batch_masks, batch_token_types])[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
